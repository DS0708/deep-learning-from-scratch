# Chapter04 신경망 학습

- 이번 장의 주제는 신경망 학습이다.
- 여기서 학습이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻한다.
- 이번 장에서는 신경망이 학습할 수 있도록 해주는 지표인 손실 함수를 소개한다.
- 이 손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습 목표이다.
- 이번 장에서는 손실 함수의 값을 가급적 작게 만드는 기법으로, 함수의 기울기를 활용하는 경사법을 소개한다.

## 4.1 데이터에서 학습한다 !
- 신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다.
- 데이터에서 학습한다는 것은 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 뜻이다.
- 이번 장에서는 신경망 학습(데이터로부터 매개변수의 값을 정하는 방법)에 대하여 설명하고 파이썬으로 
MNIST 데이터셋의 손글씨 숫자를 학습하는 코드를 구현한다.

### 4.1.1 데이터 주도 학습
- 기계학습은 데이터가 생명이다. 데이터에서 답을 찾고 데이터에서 패턴을 발견하고 데이터로 이야기를 만드는 것이
기계학습이다.
- 기계학습에서는 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려고 시도해야 한다.
- 기계학습의 두 가지 접근법이 있는데 하나는 사람이 설계를 하여 기계가 학습하는 것이고 나머지 하나는 오직 기계가
처음부터 끝까지 데이터의 특징을 생각해내어 학습한다는 것이다.

<img src="../dataset/mdImage/ch041.png" width="800">

- 이처럼 신경망(딥러닝)은 이미지를 "있는 그대로" 학습한다.
- 두 번째 접근 방식에서는 특징을 사람이 설계했지만, 신경망은 이미지에 포함된 중요한 특징까지도 "기계"가 스스로
학습할 것이다.
- 이러한 이유 때문에 딥러닝을 end-to-end machine learning이라고도 한다. 여기서 end-to-end란 '처음부터
끝까지'라는 의미로, 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻는다는 뜻을 담고 있다.
- 신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점이다.
- 예를 들어 '5'를 인식하는 문제든, '개'를 인식하는 문제든 세부사항과 관계없이 신경망은 주어진 데이터를 온전히 
학습하여 주어진 문제의 패턴을 발견하려고 시도한다.


### 4.1.2 훈련 데이터와 시험 데이터 
- 기계학습 문제는 데이터를 훈련 데이터(training data)와 시험 데이터(test data)로 나눠 학습과 실험을
수행하는 것이 일반적이다.
- 우선 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾는다. 
- 그런 다음 시험 데이터를 사용하여 앞서 훈련한 모델의 실력을 평가하는 것이다.
- 훈련 데이터와 시험 데이터를 나누는 이유는 우리는 `범용적으로 사용할 수 있는 모델을 원하기 때문이다.`
- 범용 능력을 제대로 평가하기 위해 훈련 데이터와 시험 데이터를 분리하는데, 여기서 `범용 능력이란 아직 보지 못한 데이터
  (훈련 데이터에 포함되지 않는 데이터)로도 문제를 올바르게 풀어내는 능력이다.`
- 즉 기계학습의 최종 목표는 이 범용 능력을 획득하는 것이다.
- 예를 들어, '특정인의 특정 글자'가 아닌 '임의의 사람의 임의의 글자'를 판단하는 모델을 개발해야한다.
- 만약 수중에 있는 훈련 데이터만 잘 판별한다면 그 데이터에 포함된 사람의 글씨체만 학습했을 가능성이 크다.
- 따라서 하나의 데이터셋으로만 학습과 평가를 수행하면 올바른 평가가 될 수 없다. 
- 참고로 한 데이터셋에만 지나치게 최적화된 상태를 `오버피팅(overfitting)`이라고 한다. 오버피팅을 피하는 것이
기계학습의 중요한 과제이기도 하다.


## 4.2 손실 함수
- 신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현한다.
- 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것이 목표이다.
- 신경망 학습에서 사용하는 지표는 `손실 함수 (loss fuction)`이다.
- 이 손실 함수는 일반적으로 오차제곱합과 교차 엔트로피 오차를 사용한다.
- 이 손실 함수는 '얼마나 나쁘냐'를 기준으로 하므로 이 손실 함수의 값을 0에 가까운 값으로 학습시키는 것이 목표이다.

### 4.2.1 오차제곱합 (sum of squares for error, SSE)
- 가장 많이 쓰이는 손실 함수는 오차제곱합이며 수식은 다음과 같다.
  - yi는 신경망의 출력 (신경망이 추정한 값)
  - ti는 정답 레이블
  - K는 데이터의 차원 수
  - 예를 들어 손글씨 데이터 라면 0~9까지 10개의 차원이므로 k=10 이다.
  
$$
E = \\frac{1}{2} \\sum_{i}(y_{i} - t_{i})^2
$$

```python
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```

- 다음과 같이 주어졌을 때, y는 소프트맥스 함수를 통해 출력된 값이다.
- t에서 정답을 가리키는 위치의 원소는 1로, 그 외에는 0으로 표기하는 원-핫 인코딩 방식을 사용하여 정답이 '2'임을 나타낸다.

```python
def sum_squares_error(y,t):
    return 0.5 * np.sum((y-t)**2)
```

- 여기서 인수 y와 t는 넘파이 배열이다.
- 오차제곱합을 파이썬 함수로 표현한 것임.

```python
import numpy as np

def sum_squares_error(y,t):
    return 0.5 * np.sum((y-t)**2)

y2 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
y7 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

print(sum_squares_error(np.array(y2), np.array(t)))
print(sum_squares_error(np.array(y7), np.array(t)))
```
```
결과

0.09750000000000003
0.5975
```
- 이처럼 원-핫 인코딩 방식으로 정답이 '2'인 레이블이 주어졌을 때, '2'로 예측한 값과 '7'로 예측한 결과 값을 비교해보자.
- '2'라고 정확히 예측한 결과 값이 '7'이라고 잘못 예측한 결과값보다 현저히 작은 것을 볼 수 있다.
- 즉 , 오차제곱합 기준으로는 y2의 추정 결과가 정답에 더 가까울 것으로 판단할 수 있다.


### 4.2.2 교차 엔트로피 오차 (cross entropy error, CEE)

- 또 다른 손실함수로서 교차 엔트로피 오차도 자주 이용하고 수식은 다음과 같다.
  - log는 밑이 e인 자연로그이다.
  - yi는 신경망의 출력
  - tk는 정답 레이블이다.
  - 또한 tk는 정답에 해당하는 인덱스 원소만 1이고 나머지는 0이다. (원-핫 인코딩)
  - 따라서 실질적으로 정답일 때의 추정(tk가 1일 때의 yk)의 자연로그를 계산하는 식이된다.
  - 예를 들어 정답 레이블은 '2'가 정답이라 하고 이때의 신경망 출력이 0.6이라면 교차 엔트로피 오차는
  - log0.6 = 0.51이 된다.
  - 즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.

$$
E = -\\sum_{i} t_{i} \\log(y_{i})
$$

<img src="../dataset/mdImage/CEE1.png" width="600">

- 위의 그래프 처럼 더 높은 확률로 예측할 수록 (1에 가깝게 예측할 수록) 교차 엔트로피 오차의 값은 작아진다.
- 그럼 Cross Entropy error 을 구현해보자

```python
import numpy as np

def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t*np.log(y + delta))

y2 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
y7 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

print(cross_entropy_error(np.array(y2), np.array(t)))
print(cross_entropy_error(np.array(y7), np.array(t)))
```
```
결과 

0.510825457099338
2.302584092994546
```

- np.log 를 계산할 때 아주 작은 값인 delta를 더하는 이유는 np.log()에 0을 입력하면 마이너스 무한대를
뜻하는 -inf가 되어 더 이상 계산을 진행할 수 없게 되기 때문이다.
- 첫 번째 예는 정답일 때의 출력이 0.6인 경우로, 이때의 교차 엔트로피 오차는 약 0.51이다.
- 그 다음은 정답일 때의 출력이 더 낮은 0.1인 경우로, 이때의 교차 엔트로피 오차는 무려 2.3이다.
- 즉, 결과(오차 값)가 더 작은 첫 번째 추정(y2)이 정답일 가능성이 높다고 판단한 것으로, 앞서 오차제곱합의 판단과 일치한다.


### 4.2.3 미니배치 학습

- 기계학습 문제는 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다.
- 지금까지 데이터 하나에 대한 손실함수만 생각해왔으니, 이제 훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 생각해보자.
- 데이터가 N개일 때 교차 엔트로피 오차의 수식은 다음과 같다.

$$
E = -\frac{1}{N} \sum_{n} \sum_{k} t_{nk} \log y_{nk}
$$

- tnk 는 n 번째 데이터의 k 번째 값을 의미한다. (ynk는 신경망의 출력, tnk는 정답 레이블)
- 마지막에 N으로 나누어 정규화 하고 있다.
- N으로 나눔으로써 '평균 손실 함수'를 구하는 것이다. 
- 이렇게 평균을 구해 사용하면 훈련 데이터가 몇개든 간에 상관없이 평균 손실 함수를 구할 수 있다.
- 그런데 MNIST 데이터셋은 훈련 데이터가 60,000개이기 때문에 모든 데이터를 대상으로 손실 함수의 합을 구하려면
시간이 많이 걸린다.
- 많은 데이터를 대상으로 일일이 손실 함수를 계산하는 것은 현실적이지 않기 때문에, 이런 경우 데이터 일부를 추려
전체의 '근사치'로 이용할 수 있다.
- 신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행하고 이 일부를 `미니배치(mini-batch)`라고 한다.
- 가량 60,000장의 훈련 데이터 중에서 100장을 무작위로 뽑아 그 100장만을 사용하여 학습하는 방법을 `미니배치 학습`이라고 한다.
- 그럼 이제 훈련 데이터에서 지정한 수의 데이터를 무작위로 골라내는 코드를 작성해보자
```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize=True,one_hot_label=True)

print(x_train.shape)
print(t_train.shape)

train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]

print(x_batch.shape)
print(t_batch.shape)
```
```
결과

(60000, 784)
(60000, 10)
(10, 784)
(10, 10)
```

- 이번에는 one_hot_label=True로 작성하여 원-핫 인코딩으로 배열을 얻어 냈다.
- 이처럼 np.random.choice() 함수를 사용해 훈련데이터에서 무작위로 10장만 뽑아냈다.
- x_batch와 t_batch의 shape이 (10, ~) 인 형태를 보면 알 수 있다.
- 이렇게 무작위로 선택한 인덱스를 이용해 미니배치를 뽑아냈고 손실 함수도 이 미니배치로 계산한다.

> 텔레비전 시청률도 모든 세대의 텔레비전이 아니라 선택된 일부의 가구 텔레비전만을 대상으로 구한다.<br>
> 예를 들어 경기 지방에서 무작위로 선정한 1,000 가구를 대상으로 시청률을 계측한 다음, 경기 지방 전체의 시청률로
> 근사하는 것이다. 그 1,000가구의 시청률이 전체 시청률과 정확히 일치하지는 않겠지만, 전체의 대략적인 값으로 사용할 수 있다. <br>
> 이 시청률 이야기와 마찬가지로 미니배치의 손실 함수도 일부 표본 데이터로 전체를 비슷하게 계측한다. <br>
> 즉, 전체 훈련 데이터의 대표로서 무작위로 선택한 작은 덩어리(미니배치)를 사용하는 것이다.


### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기

- 그럼 이제 미니배치 같은 배치 데이터를 지원하는 교차 엔트로피 오차를 구현해보자
- 데이터가 하나인 경우와 데이터가 배치로 묶여 입력될 경우를 모두 처리할 수 있도록 구현해보자.

```python
def cross_entropy_error(y,t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = t.reshape(1, y.size)
    
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7))/batch_size
```

- 미니배치거나 데이터 하나당 처리하는거나 모두 2차원 배열로 처리해야한다.
- 그렇기 때문에 y.ndim==1, y가 1차원일 떄, t랑 y를 모두 행이 1개인 2차원 matrix로 변환시켜준다.
- 그리고 batch_size는 식에서 N(미니배치당 데이터 개수)가 될 것이고, y가 1차원이면 1개가 된다.
- 따라서 데이터가 1개든 여러개든 간에 교차 엔트로피 오차를 구할 수 있는 함수를 만들었다.

> 위의 경우는 t가 one-hot encoding으로 주어졌을 경우다. 그럼 그렇게 주어지지 않고 '2' 나 '7'같은
> 숫자로 주어졌을 때 구한느 코드를 구현해보자

```python
def cross_entropy_error_noOneHot(y,t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = t.reshape(1, y.size)
    
    batch_size = y.shape[0]
    return -np.sum( np.log(y[np.arange(batch_size),t] + 1e-7) ) / batch_size
```
- 위의 경우는 t(정답 레이블)가 [2, 7, 8, 0] 이런식으로 주어졌을 때다.
- 만약 저런식으로 주어진다면 배치사이즈는 4가 될 것이고 y는 4 * 10 형태의 행렬로 입력이 들어왔을 것이다.
- np.arrange(batch_size)는 0부터 batch_size-1 까지의 배열을 구성하므로 [0, 1, 2, 3]이 될 것이다.
- 따라서 y[np.arange(batch_size),t] 값은 y[0,2] y[1,7] y[2,8] y[3,0] 이런 형태가 될 것이다.
- 즉 각 데이터의 정답레이블에 해당하는 신경망의 출력을 추출하여 계산하게 되는 것이다.


### 4.2.5 왜 손실 함수를 설정하는가? 
- 왜 굳이 손실 함수를 사용하여 매개변수 값을 찾는 것일까 ?
- 우리의 궁극적인 목적은 '높은 정확도'를 끌어내는 매개변수의 값을 찾는 것이다.
- 그러면 왜 '정확도'라는 지표를 놔두고 '손실 함수의 값'이라는 우회적인 방법을 택하는 것일까 ?
- 이 의문은 신경망 학습에서의 '미분'의 역할에 주목하면 된다.
- 신경망 학습에서는 최적의 매개변수(가중치와 편향)을 탐색할 때 손실 함수의 값을 가능한 한 적게 
하는 매개변수 값을 찾는다.
- 이떄 그 가중치 매개변수의 손실 함수 미분이란 `가중치 매개변수의 값을 아주 조금 변화 시켰을 때, 
손실 함수가 어떻게 변하나`라는 의미다.
- 미분 값이 양수이면 가중치 매개변수를 음의 방향으로, 미분 값이 음수이면 가중치 매개변수를 양의 방향으로 변화시켜
손실 함수의 값을 줄여 최적의 매개변수를 찾을 수 있는 것이다.
- 하지만, 신경망 학습할때 정확도를 지표로 삼으면 안된다.
- 이는 정확도는 예측이 올바르면 1, 틀리면 0 의 값을 가지는데 정확도는 예측이 맞거나 틀릴 때만 값이 변화하고,
그 사이에서는 값의 변동이 없다.
- 즉, 매개변수의 미소한 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화한다.
- 한편, 손실 함수를 지표로 삼았을떄 현재 손실함수의 값이 0.9544...와 같은 수치로 나타난다.
- 그리고 매개변수의 값이 조금 변하면 그에 반응하여 손실 함수의 값도 0.93432...와 같이 연속적으로 변하게 되는 것이다.
- 이러한 이유는 'step function'을 활성화 함수로 사용하지 않는 이유와도 들어맞는다.
- 만약 활성화 함수가 'step function'이라면 대부분의 장소에서 미분값은 0이기 때문이다.
- 따라서 활성화 함수로 'step function'대신 시그모이드 함수를 사용하며 손실 함수의 미분값을 지표로 매개변수를
학습시킨다.


## 4.3 수치 미분

경사법에는 기울기(경사)를 기준으로 나아갈 방향을 정한다. 기울기가 무엇인지, 어떤 성질이 있는지 설명하기 앞서
'미분'부터 복습해보자

### 4.3.1 미분
- 10분에 2km를 달렸을떄 속도는 2/10 = 0.2 [km/분]이다.
- 즉 1분에 0.2km 만큼의 속도로 달렸다고 해석할 수 있고 정확하게는 10분동안의 '평균 속도'를 구한 것이다.
- 미분은 '특정 순간'의 변화량을 뜻하며 10분이라는 시간을 가능한 줄여 직전 1분에 달린 거리, 직전 1초에 달린 거리
등 간격을 줄여 한 순간의 변화량(어느 순간의 속도)를 얻는 것이다.
- 이처럼 미분은 한순간의 변화량을 표시한 것이고 수식은 다음과 같다.

$$
\frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
$$

- 결국 x의 '작은 변화'가 함수 f(x)를 얼마나 변화시키느냐를 의미한다.
- 이떄 시간의 작은 변화, 즉 시간을 뜻하는 h를 한없이 0에 가깝게 한다는 의미를 lim h->0 으로 나타낸 것.
- 이를 파이썬으로 구현해보자

```python
# 나쁜 구현
def numerical_diff(f,x):
    h = 1e-50
    return (f(x+h) - f(x)) / h
```

- 이 함수는 문제점이 2가지가 있다.
  1. h에 가급적 작은 값을 대입하고 싶었기에 1e-50이라는 작은 값을 이용했으나 이 값은 rounding error
  문제를 일으킨다. 
  2. f의 차분과 관련한 것인데 진정한 미분은 x위치의 함수의 기울기에 해당하지만, 이번 구현에서 미분은
  x+h 와 x 사이의 기울기에 해당한다. 그래서 진정한 접선과 이번 구현을 통한 접선은 엄밀히 일치하진 않다.
- 문제점 첫 번쨰의 경우 h로 10^-4 정도 이용하면 해결된다.
- 문제점 두 번째의 경우 (x+h)와 (x-h)일 떄의 함수 f의 차분을 계산하면된다.
- 이를 반영해 다시 구현해보자

```python
def numerical_diff(f,x):
    h = 1e-4 #0.0001
    return (f(x+h) - f(x-h)) / (2*h)
```

> 여기에서 하는 것처럼 아주 작은 차분으로 미분하는 것을 수치미분 이라고 한다.<br>
> 한편, 수식을 전개해 미분하는 것은 해석적(analytic)이라는 말을 이용하여 '해석적 해' 혹은 '해석적으로 미분하다'
> 등으로 표현한다. 가령 y=x^2의 비분은 해석적으로는 2x이며 x=2일 때 y의 미분은 4가된다.
> 해석적 미분은 오차를 포함하지 않는 '진정한 미분'값을 구해준다.<br>
> 말 그대로 우리가 미적분 시간에 배운 미분을 해석적 미분, 진정한 미분 이라고 생각하면 된다.


### 4.3.2 수치 미분의 예

- y = 0.01x^2 + 0.1x 의 미분을 구해보자

```python
import numpy as np
import matplotlib.pylab as plt
def numerical_diff(f,x):
    h = 1e-4 #0.0001
    return (f(x+h) - f(x-h)) / (2*h)

def function_1(x):
    return 0.01*x**2 + 0.1*x

x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
plt.xlabel("x")
plt.ylabel("f(x)")
plt.plot(x,y)
plt.show()

print(numerical_diff(function_1,5))
print(numerical_diff(function_1, 10))
```
```
결과

0.1999999999990898
0.2999999999986347
```

- f(x)의 해석적 해는 0.02x + 0.1 이므로 x가 5와 10일 때의 진정한 미분은 0.2 와 0.3이다
- 결과를 보면 오차가 매우 적음을 알 수 있다.


### 4.3.3 편미분
- f(x0, x1) = x0^2 + x1^2 일때 편미분 구하기
- x0 = 3, x1 = 4 일때 x0에 대한 편미분 구하기
```python
def function_tmp1(x0):
    return x0*x0 + 4.0**2.0

print(numerical_diff(function_tmp1,3.0))
```
```
결과

6.00000000000378
```
- x0 = 3, x1 = 4 일때 x1에 대한 편미분 구하기
```python
def function_tmp2(x1):
    return 3.0**2.0 + x1*x1

print(numerical_diff(function_tmp2,4.0))
```
```
결과

7.999999999999119
```


## 4.4 기울기
- 그럼 아까 편미분 했던 것을 벡터의 결과 형태로 얻으려고 한다.
- 예를 들어 아까 x0 = 3, x1 = 4 일때 각 x0의 편미분 결과와 x1의 편미분 결과를 벡터로 얻고 싶은 것이다.
- 그럼 [6,8] 과 같은 값을 얻고 싶은 것이다.
- 코드를 보자

```python
import numpy as np

def numerical_gradient(f,x):
    h = 1e-4 # 0.001
    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성

    for idx in range(x.size):
        tmp = x[idx]
        # f(x+h) 계산
        x[idx] = tmp + h
        fxh1 = f(x)

        # f(x-h) 계산
        x[idx] = tmp - h
        fxh2 = f(x)

        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp

    return grad

def function1(x) :
    return x[0]**2 + x[1]**2

x = np.array([3.0, 4.0])

print(numerical_gradient(function1,x))

```
```
결과

[6. 8.]
```

- 함수의 구현은 좀 복잡하게 보이지만, 동작 방식은 변수가 하나일 때의 수치 미분과 거의 같다.
- 참고로 np.zeros_like(x)는 x와 형상이 같고 그 원소가 모두 0인 배열을 만든다.
- 중요한 포인트는 `기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향`이다.


### 4.4.1 경사법(경사 하강법)
- 기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다. 신경망 역시 최적의 매개변수(가중치와 편향)
를 학습 시에 찾아야 한다.
- 여기서 최적의 매개변수란 손실 함수가 최솟값이 될 때의 매개변수 값을 말한다.
- 그런 일반적인 손실 함수는 매우 복잡하며 이런 상황에서 기울기를 잘 이용해 함수의 최솟값(또는 가능한 한 작은 값)을
찾는 것이 `경사법`이다.
- 여기서 주의할 점은 `각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기`라는 것이다.
- 그러나 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 즉 그쪽으로 정말로 나아가야하는지 보장해주진 않는다.

> 경사법은 기울기가 0인 장소를 찾지만 그것이 반드시 최솟값은 아니다.
> 함수가 극솟값, 최솟값, 안정점(saddle point)이 되는 장소에서 기울기가 0인데
> 경사법은 이러한 점을 찾아 나가는 방법이다.

- 그런데 기울어진 방향이 꼭 최솟값을 가리키는 것은 아니지만, 그 방향으로 가야 함수의 값(오차 값)을 줄일 수 있다.
- `그래서 최솟값이 되는 장소를 찾는 문제에서는 기울기 정보를 단서로 나아갈 방향을 정해야 한다.`
- 여기서 `경사법`이라는 개념이 등장하는데 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다.
- 그럼 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다.
- 이렇게 해서 함수의 값을 점차 줄이는 것이 경사법(gradient method)이다.
- 이러한 경사법은 최솟값을 찾느냐, 최댓값을 찾느냐에 따라 이름이 다른데 전자를 경사 하강법(gradient descent method),
후자를 경사 상승법(gradient ascent method)라고 한다.
- 일반적으로 신경망(딥러닝) 분야에서의 경사법은 '경사 하강법'으로 등장할 때가 많다.
- 그럼 경사법을 수식으로 나타내보자.

$$
x_0 \leftarrow x_0 - \eta \frac{\partial f}{\partial x_0}
$$

$$
x_1 \leftarrow x_1 - \eta \frac{\partial f}{\partial x_1}
$$






