# Chapter6 학습 관련 기술들

- 이번 장에서는 신경망 학습의 핵심 개념들을 알아볼 것이다.
- 이번 장에서 다룰 주제는 다음과 같다.
  - 가중치 매개변수의 최적값을 탐색하는 최적화 방법
  - 가중치 매개변수 초깃값
  - 하이퍼파라미터 설정 방법
  - 오버피팅의 대응책인 가중치 감소와 드롭아웃 등의 정규화 방법
  - 배치 정규화
- 이번 장에서 설명하는 기법을 이용하면 신경망(딥러닝) 학습의 효율과 정확도를 높일 수 있다.


## 6.1 매개변수 갱신
- 신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이다.
- 이는 곧 매개변수의 최적값을 찾는 문제이며, 이러한 문제를 푸는 것을 `최적화(optimization)`이라고 한다.
- 우리는 최적화를 하기 위해 매개변수의 기울기(미분)을 이용했다.
- 매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 몇 번이고 반복해서 점점 최적의 값에 도달했다.
- 이러한 단순한 방법을 확률적 경사 하강법(SGD)이라 한다.
- 문제에 따라서는 SGD 보다 똑똑한 방법도 있으며 이제부터는 SGD의 단점을 알아본 후 SGD와는 다른 최적화 기법을 소개하겠다.

### 6.1.1 모험가 이야기
- SGD를 지도도없고 눈도 안보이는 모험가에 비유했다.
- 지도도 없고 눈도 안보이는 모험가는 땅의 기울기만을 이용하여 가장 낮은 골짜기를 찾으려고 한다.
- 지금 서 있는 장소에서 발바닥으로 기울기를 느껴 가장 크게 기울어진 방향으로 가는 것을 SGD의 전략이라고 비유 하였다.


### 6.1.2 확률적 경사 하강법(SGD)
- SGD를 복습해 보겠으며 SGD에서 가중치 갱신은 다음과 같이 이루어진다.
  - W = W - lr*dL/dW
  - W는 갱신할 가중치 매개변수
  - lr는 학습률을 의미하며 실제로는 0.01 이나 0.001과 같은 값을 미리 정해서 사용
  - dL/dW 는 가중치 매개변수에 대한 손실 함수의 기울기
  - 즉, SGD는 기울어진 방향으로 일정 거리만큼 매개변수를 조정하겠다는 최적화 방법이다.
- 파이썬 코드로 구현해보겠다.
  ```python
  class SGD:
      def __init__(self, lr=0.01):
          self.lr = lr
      
      def update(self, params, grads):
          for key in params.keys():
              params[key] -= self.lr * grads[key]
  ```

- 인수인 params와 grads는 지금까지의 신경망 구현과 마찬가지로 딕셔너리 변수이다.
- params['W1'], grads['W1']과 같은 형태로 쓰이며 각각 가중치 매개변수와 기울기를 저장하고 있다.
- 이러한 SGD 클래스를 다음과 같이 사용할 수 있다.
  ```python
  network = TwoLayerNet(...)
  optimizer = SGD()
  
  for i in range(10000):
      ...
      x_batch, t_batch = get_mini_batch(...) # 미니배치
      grads = network.gradient(x_batch, t_batch)
      params = network.params
      optimizer.update(params, grads)
      ...
  ```

- 이처럼 최적화를 담당하는 클래스를 분리해서 구현하여 기능을 모듈화 하였다.
- 예를 들어 곧이어 소개할 모멘텀이라는 최적화 기법 역시 update(params, grads)라는 공통의 메서드를 갖도록 구현한다.
- 그럼 optimizer = SGD()를 그냥 optimizer = Momentum()으로만 변경하면 SGD가 모멘텀으로 바뀌는 것이다.


> 대부분 딥러닝 프레임워크는 다양한 최적화 기법을 구현해 제공하며. 원하는 기법으로 쉽게 바꿀 수 있는 구조로 되어 있다.


### 6.1.3 SGD의 단점
- SGD는 단순하고 구현도 쉽지만, 문제에 따라서는 비효율적일 때가 있다.
- 다음 식을 봐보자 
  - f(x,y) = 1/20x^2 + y^2

- 이 그래프와 그 등고선을 보면 '밥그릇'을 x축 방향으로 늘인 듯한 모습이고, 
실제로 그 등고선은 오른쪽과 같이 x축 방향으로 늘인 타원으로 되어 있다.

<img src="../dataset/mdImage/그림6-1.jpeg">

- 함수의 기울기를 그려보면 밑의 그림 처럼 된다.

<img src="../dataset/mdImage/그림6-2.jpeg">

- 이 기울기는 y축 방향은 크고 x축 방향은 작다는 것이 특징이다.
- 말하자면 y축 방향은 가파른데 x축 방향은 완만한 것이다.
- 또한, 최솟값이 되는 좌표는 (x,y) = (0,0)이지만, 그림에서 보여주는 기울기 대부분은 (0,0) 방향을 가리키지 않는다.


- 이제 이 함수에 SGD를 적용해보자. 초깃값은 (-7.0, 2.0)으로 하겠다.

<img src="../dataset/mdImage/그림6-3.jpeg">

- 이처럼 심하게 굽이진 움직임을 보여주며, 이는 상당히 비효율적인 움직임이다.
- 즉, SGD의 단점은 비등방성(anisotropy) 함수 (방향에 따라 기울기가 달라지는 함수)에서는 탐색 경로가 비효율적이라는 것이다.
- 이럴 때는 SGD 같이 무작정 기울어진 방향으로 진행하는 단순한 방식보다 더 영리한 방식이 필요하다.
- 또한 SGD가 지그재그로 탐색하는 근본 원인은 기울어진 방향이 본래의 최솟값과 다른 방향을 가리켜서라는 점도 있다.

> 이제부터 SGD의 이러한 단점을 개선해주는 모멘텀, AdaGrad, Adam 이라는 세 방법을 소개하겠다.


### 6.1.4 모멘텀
- 모멘텀(Momentum)은 '운동량'을 뜻하는 단어로, 물리와 관계가 있다.
- 모멘텀 기법은 수식으로는 다음과 같다.
  - v = av - lr * dL/dW
  - W = W + v
  - W, lr, dL/dW는 SGD와 동일한 변수이다.
  - v가 새로 등장하는데 이는 물리에서 말하는 속도(velocity)에 해당한다.
- 이 수식은 기울기 방향으로 힘을 받아 물체가 가속된다는 물리 법칙을 나타낸다.
- 또한 av 항은 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 한다. (a는 0.9 등의 값으로 설정)
- 물리에서의 지면 마찰이나 공기 저항에 해당한다.
- 다음은 모멘텀의 구현 코드이다. (common/optimizer.py)
  ```python
  class Momentum:
      """모멘텀 SGD"""
  
      def __init__(self, lr=0.01, momentum=0.9):
          self.lr = lr
          self.momentum = momentum
          self.v = None
  
      def update(self, params, grads):
          if self.v is None:
              self.v = {}
              for key, val in params.items():
                  self.v[key] = np.zeros_like(val)
  
          for key in params.keys():
              self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]
              params[key] += self.v[key]
  ```
  - 인스턴스 변수 v가 물체의 속도이다.
  - v는 초기화 때는 아무 값도 담지 않고, 대신 update()가 처음 호출될 때 배개변수와 같은 구조의 데이터(값은 모두 0)를
  딕셔너리 변수로 저장한다.
  - 나머지 부분은 위의 모멘텀 수식을 코드로 옮긴 것이다.


- 이제 모멘텀을 사용해서 최적화 문제를 풀어보자. 결과는 다음과 같다.

<img src="../dataset/mdImage/그림6-5.png" height="300">

- 그림에서 보듯 모멘텀의 갱신 경로는 공이 경사진 바닥을 구르듯 움직인다.
- SGD와 비교하면 '지그재그 정도'가 덜한 것을 알 수 있다.
- 이는 x축의 힘은 아주 작지만 방향은 변하지 않아서 한 방향으로 일정하게 가속하기 때문이다.
- 거꾸로 y축의 힘은 크지만 위아래로 번갈아 받아서 상충하여 y축 방향의 속도는 안정적이지 않다.
- 전체적으로는 SGD보다 x축 방향으로 빠르게 다가가 지그재그 움직임이 줄어든다.


### 6.1.5 AdaGrad
- 신경망 학습에서는 학습률 값이 중요하다.
- 이 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 학습이 제대로 이뤄지지 않는다.
- 이 학습률을 정하는 효과적 기술로 `학습률 감소(learning rate decay)`가 있다.
- 이는 학습을 진행하면서 학습률을 점차 줄이는 방법이다.
- 말 그대로 처음에는 크게 학습하다가 조금씩 작게 학습한다는 방법으로 신경망 학습에 자주 쓰인다.
- 학습률을 서서히 낮추는 가장 간단한 방법은 매개변수 '전체'의 학습률 값을 일괄적으로 낮추는 것이다.
- 이를 더욱 발전시킨 것이 AdaGrad이며, AdaGrad는 '각각의' 매개변수에 '맞춤형' 값을 만들어준다.
- AdaGrad는 개별 매개변수에 adaptive(적응적으로) 학습률을 조정하면서 학습을 진행한다.
- 수식은 다음과 같다.

<img src="../dataset/mdImage/식6.5.png">

- 새로 h라는 변수가 등장하는데 기존 기울기 값을 제곱하여 계속 더해준다.
- 그리고 매개변수를 갱신할 때(두번 째 식에서), h에 루트를 씌어 역수를 취한 값을 곱해 학습률을 조정한다.
- 즉, 매개변수의 원소 중에서 많이 움직인(기울기가 커서 크게 갱신된) 원소는 학습률이 낮아진다는 뜻이다.
- 다시 말해 학습률 감소가 매개변수의 원소마다 다르게 적용됨을 뜻한다.
- 참고로 h식에서 dL/dW 사이의 기호는 행렬의 원소별 곱셈을 의미한다.

> AdaGrad는 과거의 기울기를 제곱하여 계속 더해간다. 그래서 학습을 진행할 수록 갱신 강도가 약해지며 무한히 학습
> 한다면 어느 순간 갱신량이 0이 되어 전혀 갱신되지 않다. 이 문제를 개선한 기법으로서 RMSProp라는 방법이 있다.
> 이 방법은 과거의 모든 기울기를 균일하게 더해가는 것이 아닌, 먼 과거의 기울기는 서서히 잊고 새로운 정보를 크게 반영하며 이를
> 지수이동평균(Exponential Moving Average, EMA)라 하여, 과거 기울기의 반영 규모를 기하급수적으로 감소시킨다.


- 그럼 AdaGrad의 구현을 보자 
  ```python
  class AdaGrad:
      """AdaGrad"""
  
      def __init__(self, lr=0.01):
          self.lr = lr
          self.h = None
  
      def update(self, params, grads):
          if self.h is None:
              self.h = {}
              for key, val in params.items():
                  self.h[key] = np.zeros_like(val)
  
          for key in params.keys():
              self.h[key] += grads[key] * grads[key]
              params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
  ```
  - 여기서 주의할 부분은 마지막 줄에서 1e-7이라는 작은 값을 더하는 부분인데 이 작은 값은 self.h[key]가
  0일 수도 있어 0으로 나눠주는 부분을 예외처리 하기 위해 더한 것이다.
  - 대부분 딥러닝 프레임워크에서는 이 값도 인수로 설정할 수 있다.


- 그럼 이렇게 구현한 AdaGrad를 사용해서 최적화 문제를 풀어보자.

<img src="../dataset/mdImage/그림6-6.png">

- 위의 그림을 보면 최솟값을 향해 효율적으로 움직이는 것을 볼 수 있다.
- y축 방향은 기울기가 커서 처음에는 크게 움직이지만, 그 큰 움직임에 비례해 갱신 정도도 큰 폭으로 작아지도록
조정된다.
- 그래서 y축 방향으로 갱신 강도가 빠르게 약해지고, 지그재그 움직임이 줄어든다.


### 6.1.6 Adam

- 모멘텀은 공이 기울어진 바닥을 구르는 듯한 움직임을 보였다.
- AdaGrad는 매개변수의 원소마다 적응적으로 갱신 정도를 조정했다.
- 이제 이 두 기법을 융합해보자는 생각에서 만들어진 기법이 바로 `Adam이다.`


- Adam은 2015년에 제안된 새로운 방법으로 이론은 조금 복잡하지만 직관적으로는 모멘텀과 AdaGrad를 융합한 듯한 방법이다.
- 이 두 방법의 이점을 조합했다면 매개변수 공간을 효율적으로 탐색해줄 것으로 기대할 수 있을 것이다.
- 또한 하이퍼파라미터의 '편향 보정'이 진행된다는 점도 Adam의 특징이다.
- 이제 더 깊게 파고들진 않을 것이고 궁금한 사람은 원논문을 참고하고, commom/optimizer.py에 구현한 Adam클래스를
보면 된다.
- 그럼 Adam을 사용하여 최적화 문제를 풀어보자.

<img src="../dataset/mdImage/그림6-7.png">

- 다음 그림과 같이 Adam 갱신 과정도 그릇 바닥을 구르듯 움직인다.
- 모멘텀과 비슷한 패턴인데, 모멘텀 때보다 공의 좌우 흔들림이 적다.
- 이는 학습의 갱신 강도를 Adaptive하게 조정해서 얻은 결과이다.


### 6.1.7 어느 갱신 방법을 이용할 것인가?

- 이제까지 매개변수의 갱신 방법 (최적화 방법) 4가지를 살펴봤다. (비교 소스 코드 : Ch06/optimizer_compare_naive.py)
- 전체 그림만 놓고 보면 AdaGrad가 가장 나은 것 같지만 사실 그 결과는 풀어야 할 문제가 무엇이냐에 따라 달라지므로
주의 해야한다.
- 또한 학습률 등의 하이퍼파라미터를 어떻게 설정하느냐에 따라서도 결과가 바뀐다.
- 그렇다면 SGD, 모멘텀, AdaGrad, Adam의 네 후보 중 어느 것을 채택하면 될까 ? 
- 유감스럽지만 모든 문제에서 항상 뛰어난 기법이라는 것은 아직까지는 없다.
- 각자의 장단점이 있어 잘 푸는 문제와 못 푸는 문제가 있을 뿐이다.
- 지금도 많은 연구에서 SGD를 사용하고 있으며 모멘텀과 AdaGrad도 시도해볼 만한 가치가 충분하다.
- 요즘에는 많인 분이 Adam을 만족해하며 쓰는 것 같다.
- 이 책에서는 주로 SGD와 Adam을 사용하지만, 각자의 상황을 고려해 여러 가지를 시도해보면 좋을 것이다.


### 6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교
- 손글씨 숫자 인식을 대상으로 지금까지 설명한 네 기법을 비교해보자.
- 각 방법의 학습 진도가 얼마나 다른지를 아래 그림으로 나타냈다. (소스코드는 : Ch06/optimizer_compare_mnist.py)

<img src="../dataset/mdImage/그림6-9.png">

- 이 실험은 각 층이 100개의 뉴런으로 구성된 5층 신경망에서 ReLU를 활성화 함수로 사용해 측정했다.
- SGD의 학습 진도가 가장 느리고 나머지 세 기법의 진도는 비슷한데 자세히 보면 AdaGrad가 조금 더 빠른 것 같다.
- 이 실험에서 주의할 점은 하이퍼파라미터인 학습률과 신경망의 구조(층 깊이 등)에 따라 결과가 달라진다는 것이다.
- 다만 일반적으로 SGD보다 다른 세 기법이 빠르게 학습하고, 때로는 최종 정확도도 높게 나타난다.


## 6.2 가중치의 초깃값
- 신경망 학습에서 특히 중요한 것은 가중치 초기값이다.
- 가중치의 초깃값을 무엇으로 설정하느냐가 신경망 학습의 성패가 가르는 일이 실제로 자주 있다.
- 이번 절에서는 권장 초깃값에 대해 설명하고 실험을 통해 신경망 학습이 신속하게 이뤄지는 모습을 확인할 것이다.


### 6.2.1 초깃값을 0으로 한다면?

- `가중치 감소(weight decay)` 기법을 소개하려고 한다.
- 가중치 감소란 오버피팅을 억제해 범용 성능을 높이는 테크닉이다.
- 간단히 말하면, 가중치 매개변수의 가중치 값을 작게 하여 오버피팅이 일어나지 않게 하는 것이다.


- 가중치를 작게 만들고 싶으면 초깃값을 최대한 작은 값에서 시작하면 된다.
- 지금까지의 초깃값은 0.01 * np.random.randn(10,100) 처럼 정규분포에서 생성되는 값을 0.01배 한 작은 값을 사용했다.
- 그렇다면 모두 0으로 설정하면 어떨까 ?
- 가중치 초깃값을 모두 0으로 설정하면 학습이 올바르게 이뤄지지 않는다. (정확히는 가중치를 모두 같은 값으로 설정해서는 안된다.)
- 이유는 다음과 같다.
  - 오차역전파법에서 모든 가중치의 값이 똑같이 갱신되기 때문이다.
  - 예를 들어 2층 신경망에서 첫 번째와 두 번째 층의 가중치가 0이라고 해보자
  - 순전파 떄는 입력층의 가중치가 0이기 때문에 두 번째 층의 뉴런에 모두 같은 값이 전달된다.
  - 두 번째 층의 모든 뉴런에 같은 값이 입력된다는 것은 역전파 때 두 번째 층의 가중치가 모두 똑같이 갱신된다는 말이다.
  - 이는 곱셈 노드의 역전파를 떠올려보면 알 수 있다.
  - 그래서 가중치들을 같은 초깃값에서 시작하면 가중치를 여러 개 갖는 의미가 사라진다.

> 이러한 가중치의 대칭적인 구조(가중치가 고르게 되어버리는 상황)를 무너뜨리기 
> 위해서는 초깃값을 무작위로 설정해야 한다.


### 6.2.2 은닉층의 활성화값 분포
- 은닉층의 활성화 함수의 출력 데이터의 분포를 관찰하면 중요한 정보를 얻을 수 있다.
- 이번 절에서는 가중치의 초깃값에 따라 은닉층 활성화값들이 어떻게 변화하는지 간단한 실험을 하겠다.
- 구체적으로는 활성화 함수로 시그모이드 함수를 사용하는 5층 신경망에 무작위로 생성한 입력 데이터를 흘리며
각 층의 활성화값 분포를 히스토그램으로 그려보겠다.
- 이 실험을 위한 전체 코드는 Ch06/weight_init_activation_histogram.py에 있다.

```python
import numpy as np
import matplotlib.pyplot as plt


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def ReLU(x):
    return np.maximum(0, x)


def tanh(x):
    return np.tanh(x)


input_data = np.random.randn(1000, 100)  # 1000개의 데이터
node_num = 100  # 각 은닉층의 노드(뉴런) 수
hidden_layer_size = 5  # 은닉층이 5개
activations = {}  # 이곳에 활성화 결과를 저장

x = input_data

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i - 1]

    # 초깃값을 다양하게 바꿔가며 실험해보자！
    w = np.random.randn(node_num, node_num) * 1
    # w = np.random.randn(node_num, node_num) * 0.01
    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)
    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)

    a = np.dot(x, w)

    # 활성화 함수도 바꿔가며 실험해보자！
    z = sigmoid(a)
    # z = ReLU(a)
    # z = tanh(a)

    activations[i] = z

# 히스토그램 그리기
for i, a in activations.items():
    plt.subplot(1, len(activations), i + 1)
    plt.title(str(i + 1) + "-layer")
    if i != 0: plt.yticks([], [])
    # plt.xlim(0.1, 1)
    # plt.ylim(0, 7000)
    plt.hist(a.flatten(), 30, range=(0, 1))
plt.show()
```

- 코드는 다음과 같으며, 은닉층이 5개가 있고 각 층의 뉴런의 개수는 100개이다.
- 입력 데이터로 1,000개의 데이터를 정규분포로 무작위로 생성하여 5층 신경망에 넣는다.
- 활성화 함수는 시그모이드 함수이며 각 층의 활성화 결과를 activations 변수에 저장했다.
- 표준편차가 1인 정규분포를 이용했으며, 이 분포된 정도(표준편차)를 바꿔가며 활성화값들의 분포가 어떻게 변화하는지 관찰
하는 것이 이번 실험의 목표입니다.
- 그럼 각 표준편차를 다르게 하여 비교해보겠다.

<br><br>

#### 표준편차가 1인 정규분포 사용
<img src="../dataset/mdImage/그림6-10.png">

- 각 층의 활성화값들이 0과 1에 치우쳐 분포되어 있다.
- 여기에서 사용한 시그모이드 함수는 출력이 0과 1에 가까워질 수록 미분은 0에 다가간다.
- 그래서 역전파의 기울기는 점점 작아지다가 사라지며 이를 `기울기 소실(gradient vanishing)`이라고 한다.


#### 표준편차가 0.01인 정규분포 사용
<img src="../dataset/mdImage/그림6-11.png">

- 활성화 함수의 출력값이 0.5 부근에 집중되었다. 
- 기울기 소실 문제는 일어나지 않지만 활성화 값들이 치우쳤다는 것은 표현력 관점에서 큰 문제가 있다.
- 무슨 이야기냐 하면, 다수의 뉴런이 거의 같은 값을 출력하고 있으니 뉴런을 여러 개 둔 의미가 없어진다는 것이다.
- 예를 들어 100개가 거의 같은 출력을 하니 뉴런 1개짜리와 별반 다를게 없다는 것이다.
- 그래서 활성화 값들이 치우치면 `표현력을 제한`한다는 관점에서 문제가 된다.

> 각 층의 활성화 값은 적당히 고루 분포되어야 한다. 층과 층 사이에 적당하게 다양한 데이터가 흐르게 해야 신경망
> 학습이 효율적으로 이뤄지기 때문이다. 반대로 치우친 데이터가 흐르면 기울기 소실이나 표현력 제한 문제에 빠져서
> 학습이 잘 이뤄지지 않는 경우가 생긴다.


#### Xavier 초깃값
- Xavier 초깃값이란 사비에르 글로로트(Xavier Glorot)와 요슈아 벤지오(Yoshua Bengio)의 논문에서 권장하는
가중치 초깃값이다.
- 현재 Xavier 초깃값은 일반적인 딥러닝 프레임워크들이 표준적으로 이용하고 있다.
- 이 논문은 각 층의 활성화값들을 광범위하게 분포시킬 목적으로 가중치의 적절한 분포를 찾고자 했다.
- 그리고 앞 계층의 노드가 n개라면 표준편차가 1/root(n) 인 분포를 사용하면 된다는 결론을 내렸다.

<img src="../dataset/mdImage/그림6-12.png">

- Xavier 초깃값을 사용하면 앞 층에 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼진다.
- 이제 Xavier 초깃값을 써서 실험한 결과를 보자.
- 참고로 여기 구현에서는 모든 층의 노드 수가 100개라고 단순화 했다.

<img src="../dataset/mdImage/그림6-13.png">

- 이 결과를 보면 층이 깊어지면서 형태가 다소 일그러지지만, 앞에서 본 방식보다는 확실히 넓게 분포됨을 알 수 있다.
- 각 층에 흐르는 데이터는 적당히 퍼져 있으므로, 시그모이드 함수의 표현력도 제한받지 않고 학습이 효율적으로 이뤄질 것으로 기대된다.

> 오른쪽으로 갈수록 약간씩 일그러지고 있는데 이것은 시그모이드함수 대신에 tanh(쌍곡선 함수) 함수를 이용하면 개선된다.
> tanh함수를 이용하면 말끔한 종 모양으로 분포된다. tanh 함수도 sigmoid 함수와 같은 'S'자 모양 곡선 함수이다.
> 다만 tanh 함수가 원점(0,0)에서 대칭인 S 곡선인 반면, sigmoid 함수는 (x,y) = (0, 0.5)에서 대칭인 S 곡선이다.
> 활성화 함수용으로는 원점에서 대칭인 함수가 좋다고 한다. 참고용으로 tanh 그래프 그림을 추가하였다.

<img src="../dataset/mdImage/tanh.png">


### 6.2.3 ReLU를 사용할 떄의 가중치 초깃값
- Xavier 초깃값은 활성화 함수가 선형인 것을 전제로 이끈 결과이다.
- 시그모이드 함수와 tanh 함수는 좌우 대칭이라 중앙 부근이 선형인 함수로 볼 수 있다.
- 반면 ReLU를 이용할 때는 ReLU에 특화된 초깃값을 이용해야 하는데 이 특화된 초깃값을 He 초깃값 이라고 한다.
- He초깃값은 앞 계층의 노드가 n개일 떄, 표준편차가 root(2/n)인 정규분포를 사용한다.
- 참고로 Xavier 초깃값이 root(1/n)이었다.
- ReLU는 음의 영역이 0이라서 더 넓게 분포시키기 위해 2배의 계수가 필요하다고 (직감적으로) 해석할 수 있다.
- 이제 활성화 함수를 바꿔, 각 초깃값을 비교해 보겠다.
- 결과는 아래 그림과 같다.

<img src="../dataset/mdImage/그림6-14.png">

- 결과를 보면 std = 0.01일 때의 각 층의 활성화값들은 아주 작은 값이다.
- 신경망에 아주 작은 데이터가 흐른다는 것은 역전파 때 가중치의 기울기 역시 작아진다는 뜻이다.
- 이는 중대한 문제이며 실제로 학습이 거의 이뤄지지 않을 것이다.
- 이어서 Xavier 초깃값 결과를 보면 이쪽은 층이 깊어지면서 치우침이 조금씩 커진다.
- 실제로 층이 깊어지면 활성화값들의 치우침도 커지고, 학습할 때 '기울기 소실'문제를 일으킨다.
- 마지막으로 He 초깃값은 모든 층에서 균일하게 분포된다.
- 층이 깊어져도 분포가 균일하게 유지되기에 역전파 때도 적절한 값이 나올 것으로 기대할 수 있다.

> 이 실험 결과를 바탕으로, 활성화 함수로 ReLU를 사용할 때는 He 초깃값을, 시그모이드나 tanh 등의 S자 모양
> 곡선일 때는 Xavier 초깃값을 사용하겠다. 이것이 현재까지의 최선이다.


### 6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교
- 이번에는 '실제' 데이터를 가지고 가중치의 초깃값을 주는 방법이 신경망 학슴에 얼마나 영향을 주는지 알아보겠다.
- 지금까지 시험해 본 세 경우 (std = 0.01, Xavier 초깃값, He 초깃값) 모두를 실험해보겠다.
- 소스코드는 Ch06/weight_init_compare.py 에 있으며 결과부터 확인해보겠다.

<img src="../dataset/mdImage/그림6-15.png">

- 이 실험은 층별 뉴런 수가 100개인 5층 신경망이다.
- std=0.01 일 때는 학습이 전혀 이뤄지지 않는다.
- 앞서 활성화값의 분포에서 본 것처럼 순전파 때 너무 작은 값(0 근처로 밀집한 데이터)가 흐르기 때문이다.
- 그로 인해 역전파 떄의 기울기도 작아져 가중치가 거의 갱신되지 않는 것이다.
- 반대로 Xavier와 He 초깃값의 경우는 학습이 순조롭게 이뤄지고 있다.
- 다만 학습 진도는 He 초깃값 쪽이 더 빠르다.

> 지금까지 살펴보았듯 가중치 초깃값은 신경망 학슴에 아주 중요한 포인트다.


## 6.3 배치 정규화

















