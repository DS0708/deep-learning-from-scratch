# Chapter6 학습 관련 기술들

- 이번 장에서는 신경망 학습의 핵심 개념들을 알아볼 것이다.
- 이번 장에서 다룰 주제는 다음과 같다.
  - 가중치 매개변수의 최적값을 탐색하는 최적화 방법
  - 가중치 매개변수 초깃값
  - 하이퍼파라미터 설정 방법
  - 오버피팅의 대응책인 가중치 감소와 드롭아웃 등의 정규화 방법
  - 배치 정규화
- 이번 장에서 설명하는 기법을 이용하면 신경망(딥러닝) 학습의 효율과 정확도를 높일 수 있다.


## 6.1 매개변수 갱신
- 신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것이다.
- 이는 곧 매개변수의 최적값을 찾는 문제이며, 이러한 문제를 푸는 것을 `최적화(optimization)`이라고 한다.
- 우리는 최적화를 하기 위해 매개변수의 기울기(미분)을 이용했다.
- 매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 값을 갱신하는 일을 몇 번이고 반복해서 점점 최적의 값에 도달했다.
- 이러한 단순한 방법을 확률적 경사 하강법(SGD)이라 한다.
- 문제에 따라서는 SGD 보다 똑똑한 방법도 있으며 이제부터는 SGD의 단점을 알아본 후 SGD와는 다른 최적화 기법을 소개하겠다.

### 6.1.1 모험가 이야기
- SGD를 지도도없고 눈도 안보이는 모험가에 비유했다.
- 지도도 없고 눈도 안보이는 모험가는 땅의 기울기만을 이용하여 가장 낮은 골짜기를 찾으려고 한다.
- 지금 서 있는 장소에서 발바닥으로 기울기를 느껴 가장 크게 기울어진 방향으로 가는 것을 SGD의 전략이라고 비유 하였다.


### 6.1.2 확률적 경사 하강법(SGD)
- SGD를 복습해 보겠으며 SGD에서 가중치 갱신은 다음과 같이 이루어진다.
  - W = W - L*dL/dW
  - W는 갱신할 가중치 매개변수
  - L는 학습률을 의미하며 실제로는 0.01 이나 0.001과 같은 값을 미리 정해서 사용
  - dL/dW 는 가중치 매개변수에 대한 손실 함수의 기울기
- 파이썬 코드로 구현해보겠다.

```python
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
    
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```

- 인수인 params와 grads는 지금까지의 신경망 구현과 마찬가지로 딕셔너리 변수이다.
- params['W1'], grads['W1']과 같은 형태로 쓰이며 각각 가중치 매개변수와 기울기를 저장하고 있다.
- 이러한 SGD 클래스를 다음과 같이 사용할 수 있다.

```python
network = TwoLayerNet(...)
optimizer = SGD()

for i in range(10000):
    ...
    x_batch, t_batch = get_mini_batch(...) # 미니배치
    grads = network.gradient(x_batch, t_batch)
    params = network.params
    optimizer.update(params, grads)
    ...
```

- 이처럼 최적화를 담당하는 클래스를 분리해서 구현하여 기능을 모듈화 하였다.
- 예를 들어 곧이어 소개할 모멘텀이라는 최적화 기법 역시 update(params, grads)라는 공통의 메서드를 갖도록 구현한다.
- 그럼 optimizer = SGD()를 그냥 optimizer = Momentum()으로만 변경하면 SGD가 모멘텀으로 바뀌는 것이다.


> 대부분 딥러닝 프레임워크는 다양한 최적화 기법을 구현해 제공하며. 원하는 기법으로 쉽게 바꿀 수 있는 구조로 되어 있다.


### 6.1.3 SGD의 단점